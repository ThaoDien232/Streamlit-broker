import os
import json
import requests
import streamlit as st
from datetime import datetime, time
from dotenv import load_dotenv

# Load API key from .env
load_dotenv()
API_KEY = os.getenv("SONAR_API_KEY")
API_URL = "https://api.perplexity.ai/chat/completions"

# Trusted Vietnamese finance news sites
VN_SOURCES = [
    "cafef.vn",
    "vnexpress.net",
    "vneconomy.vn",
    "fireant.vn",
]

# ------------- Helper Functions ------------------

def fetch_news(tickers, domains=VN_SOURCES, recency="day", context_size="medium"):
    """Call Perplexity Sonar API and fetch Vietnamese news summaries about tickers."""
    if not API_KEY:
        st.error("âŒ Missing SONAR_API_KEY. Please add it to your .env file.")
        return None, None, []

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    # Build enhanced search terms with both tickers and company names
    enhanced_search_terms = []
    for ticker in tickers:
        search_terms = get_search_terms(ticker)
        enhanced_search_terms.append(search_terms)
    
    # Build user prompt based on recency period
    if recency == "day":
        time_period = "2 ngÃ y gáº§n nháº¥t"

        prompt = (
            f"TÃ¬m Táº¤T Cáº¢ tin tá»©c vá» cÃ¡c cÃ´ng ty sau trong {time_period}: {', '.join(tickers)}. "
            f"TÃ¬m kiáº¿m cáº£ MÃƒ Cá»” PHIáº¾U vÃ  TÃŠN CÃ”NG TY Ä‘á»ƒ cÃ³ káº¿t quáº£ chÃ­nh xÃ¡c nháº¥t. "
            f"CHá»ˆ bÃ¡o cÃ¡o tin tá»©c cÃ³ chá»©a tÃªn mÃ£ cá»• phiáº¿u hoáº·c tÃªn cÃ´ng ty cá»¥ thá»ƒ. "
            f"CHá»ˆ bÃ¡o cÃ¡o tin tá»©c náº¿u cÃ³ bÃ i bÃ¡o hoáº·c thÃ´ng bÃ¡o Cá»¤ THá»‚ tá»« cÃ¡c nguá»“n bÃ¡o chÃ­ chÃ­nh thá»‘ng "
            f"(CafeF, VnExpress, VnEconomy, hoáº·c cÃ´ng bá»‘ thÃ´ng tin trÃªn website cÃ´ng ty). "
            f"TUYá»†T Äá»I KHÃ”NG Ä‘Æ°á»£c tá»± suy Ä‘oÃ¡n hoáº·c táº¡o ra tin tá»©c náº¿u khÃ´ng tÃ¬m tháº¥y bÃ i bÃ¡o. "
            f"TUYá»†T Äá»I Bá» QUA cÃ¡c tin tá»©c khÃ´ng liÃªn quan Ä‘áº¿n chá»§ Ä‘á» cá»¥ thá»ƒ sau: "
            f"- biáº¿n Ä‘á»™ng giÃ¡ cá»• phiáº¿u (tÄƒng, giáº£m, % thay Ä‘á»•i, má»©c giÃ¡, vá»‘n hÃ³a), "
            f"- giao dá»‹ch khá»‘i ngoáº¡i, tá»± doanh, dÃ²ng tiá»n, khá»‘i lÆ°á»£ng giao dá»‹ch, "
            f"- phÃ¢n tÃ­ch ká»¹ thuáº­t, khuyáº¿n nghá»‹ Ä‘áº§u tÆ°, tÃ¢m lÃ½ thá»‹ trÆ°á»ng, "
            f"- cÃ¡c chá»§ Ä‘á» xÃ£ há»™i, chÃ­nh trá»‹, kinh táº¿ vÄ© mÃ´ khÃ´ng liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ´ng ty, "
            f"- tin tá»©c vá» ngÃ nh nghá» nÃ³i chung mÃ  khÃ´ng nÃªu tÃªn cÃ´ng ty cá»¥ thá»ƒ. "
            f"Æ¯U TIÃŠN VÃ€ Báº®T BUá»˜C GIá»® Láº I cÃ¡c tin tá»©c quan trá»ng vá»: "
            f"- káº¿t quáº£ kinh doanh (doanh thu, lá»£i nhuáº­n, bÃ¡o cÃ¡o tÃ i chÃ­nh, káº¿t quáº£ quÃ½/nÄƒm), "
            f"- thay Ä‘á»•i nhÃ¢n sá»± cáº¥p cao (bá»• nhiá»‡m, tá»« nhiá»‡m, thay tháº¿ CEO/lÃ£nh Ä‘áº¡o), "
            f"- táº¥t cáº£ hoáº¡t Ä‘á»™ng tÄƒng vá»‘n: phÃ¡t hÃ nh cá»• phiáº¿u riÃªng láº», chÃ o bÃ¡n cá»• phiáº¿u cho cá»• Ä‘Ã´ng hiá»‡n há»¯u, IPO, "
            f"- phÃ¡t hÃ nh trÃ¡i phiáº¿u, huy Ä‘á»™ng vá»‘n, tÄƒng vá»‘n Ä‘iá»u lá»‡, "
            f"- hoáº¡t Ä‘á»™ng M&A (sÃ¡p nháº­p, mua bÃ¡n, Ä‘áº§u tÆ° chiáº¿n lÆ°á»£c), "
            f"- dá»± Ã¡n Ä‘áº§u tÆ° lá»›n, má»Ÿ rá»™ng kinh doanh, thÃ nh láº­p cÃ´ng ty con, "
            f"- há»£p tÃ¡c chiáº¿n lÆ°á»£c, kÃ½ káº¿t há»£p Ä‘á»“ng lá»›n, "
            f"- vÆ°á»›ng máº¯c phÃ¡p lÃ½, xá»­ pháº¡t, Ä‘iá»u tra cá»§a cÆ¡ quan quáº£n lÃ½, "
            f"- thÃ´ng bÃ¡o tá»« ÄHCÄ, quyáº¿t Ä‘á»‹nh cá»§a HÄQT, nghá»‹ quyáº¿t quan trá»ng. "
            f"QUAN TRá»ŒNG: KHÃ”NG Bá» SÃ“T báº¥t ká»³ tin tá»©c nÃ o vá» 'chÃ o bÃ¡n cá»• phiáº¿u riÃªng láº»', 'private placement', 'tÄƒng vá»‘n', 'huy Ä‘á»™ng vá»‘n'. "
            f"Náº¿u KHÃ”NG cÃ³ tin tá»©c nÃ o phÃ¹ há»£p cho má»™t cÃ´ng ty, HOÃ€N TOÃ€N Bá» QUA cÃ´ng ty Ä‘Ã³ (khÃ´ng viáº¿t dÃ²ng 'khÃ´ng cÃ³ tin tá»©c'). "
            f"Äá»‹nh dáº¡ng: Má»—i tin tá»©c lÃ  Má»˜T DÃ’NG RIÃŠNG, báº¯t Ä‘áº§u báº±ng **MÃƒ Cá»” PHIáº¾U**: theo sau lÃ  ná»™i dung vÃ  LUÃ”N Káº¾T THÃšC Báº°NG LINK TRá»°C TIáº¾P Ä‘áº¿n bÃ i bÃ¡o hoáº·c thÃ´ng tin gá»‘c. "
            f"Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t."
        )
    else:  # week or other periods
        time_period = "2 tuáº§n gáº§n nháº¥t" if recency == "week" else "thá»i gian gáº§n Ä‘Ã¢y"

        prompt = (
            f"TÃ¬m Táº¤T Cáº¢ tin tá»©c vá» cÃ¡c cÃ´ng ty sau trong {time_period}: {', '.join(tickers)}. "
            f"TÃ¬m kiáº¿m cáº£ MÃƒ Cá»” PHIáº¾U vÃ  TÃŠN CÃ”NG TY Ä‘á»ƒ cÃ³ káº¿t quáº£ chÃ­nh xÃ¡c nháº¥t. "
            f"CHá»ˆ bÃ¡o cÃ¡o tin tá»©c cÃ³ chá»©a tÃªn mÃ£ cá»• phiáº¿u hoáº·c tÃªn cÃ´ng ty cá»¥ thá»ƒ. "
            f"TUYá»†T Äá»I Bá» QUA cÃ¡c tin tá»©c khÃ´ng liÃªn quan Ä‘áº¿n chá»§ Ä‘á» cá»¥ thá»ƒ sau: "
            f"- biáº¿n Ä‘á»™ng giÃ¡ cá»• phiáº¿u (tÄƒng, giáº£m, % thay Ä‘á»•i, má»©c giÃ¡, vá»‘n hÃ³a), "
            f"- giao dá»‹ch khá»‘i ngoáº¡i, tá»± doanh, dÃ²ng tiá»n, khá»‘i lÆ°á»£ng giao dá»‹ch, "
            f"- phÃ¢n tÃ­ch ká»¹ thuáº­t, khuyáº¿n nghá»‹ Ä‘áº§u tÆ°, tÃ¢m lÃ½ thá»‹ trÆ°á»ng, "
            f"- cÃ¡c hoáº¡t Ä‘á»™ng PR, marketing, sá»± kiá»‡n triá»ƒn lÃ£m, há»™i chá»£, tÃ i trá»£, giáº£i thÆ°á»Ÿng, CSR, "
            f"- cÃ¡c chá»§ Ä‘á» xÃ£ há»™i, chÃ­nh trá»‹, kinh táº¿ vÄ© mÃ´ khÃ´ng liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ´ng ty, "
            f"- tin tá»©c vá» ngÃ nh nghá» nÃ³i chung mÃ  khÃ´ng nÃªu tÃªn cÃ´ng ty cá»¥ thá»ƒ. "
            f"Æ¯U TIÃŠN VÃ€ Báº®T BUá»˜C GIá»® Láº I cÃ¡c tin tá»©c quan trá»ng vá»: "
            f"- káº¿t quáº£ kinh doanh (doanh thu, lá»£i nhuáº­n, bÃ¡o cÃ¡o tÃ i chÃ­nh, káº¿t quáº£ quÃ½/nÄƒm), "
            f"- thay Ä‘á»•i nhÃ¢n sá»± cáº¥p cao (bá»• nhiá»‡m, tá»« nhiá»‡m, thay tháº¿ CEO/lÃ£nh Ä‘áº¡o), "
            f"- táº¥t cáº£ hoáº¡t Ä‘á»™ng tÄƒng vá»‘n: phÃ¡t hÃ nh cá»• phiáº¿u riÃªng láº», chÃ o bÃ¡n cá»• phiáº¿u cho cá»• Ä‘Ã´ng hiá»‡n há»¯u, IPO, "
            f"- phÃ¡t hÃ nh trÃ¡i phiáº¿u, huy Ä‘á»™ng vá»‘n, tÄƒng vá»‘n Ä‘iá»u lá»‡, "
            f"- hoáº¡t Ä‘á»™ng M&A (sÃ¡p nháº­p, mua bÃ¡n, Ä‘áº§u tÆ° chiáº¿n lÆ°á»£c), "
            f"- dá»± Ã¡n Ä‘áº§u tÆ° lá»›n, má»Ÿ rá»™ng kinh doanh, thÃ nh láº­p cÃ´ng ty con, "
            f"- há»£p tÃ¡c chiáº¿n lÆ°á»£c, kÃ½ káº¿t há»£p Ä‘á»“ng lá»›n, "
            f"- vÆ°á»›ng máº¯c phÃ¡p lÃ½, xá»­ pháº¡t, Ä‘iá»u tra cá»§a cÆ¡ quan quáº£n lÃ½, "
            f"- thÃ´ng bÃ¡o tá»« ÄHCÄ, quyáº¿t Ä‘á»‹nh cá»§a HÄQT, nghá»‹ quyáº¿t quan trá»ng. "
            f"QUAN TRá»ŒNG: KHÃ”NG Bá» Sá»T báº¥t ká»³ tin tá»©c nÃ o vá» 'chÃ o bÃ¡n cá»• phiáº¿u riÃªng láº»', 'private placement', 'tÄƒng vá»‘n', 'huy Ä‘á»™ng vá»‘n'. "
            f"Náº¿u khÃ´ng tÃ¬m tháº¥y tin tá»©c phÃ¹ há»£p cho cÃ´ng ty nÃ o, bá» qua cÃ´ng ty Ä‘Ã³. "
            f"Äá»‹nh dáº¡ng: Má»—i tin tá»©c lÃ  Má»˜T DÃ’NG RIÃŠNG, báº¯t Ä‘áº§u báº±ng **MÃƒ Cá»” PHIáº¾U**: theo sau lÃ  ná»™i dung vÃ  nguá»“n. "
            f"Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t."
        )

    # Add date-based seed for consistency within the same day
    import hashlib
    from datetime import datetime
    
    current_date_str = datetime.now().strftime("%Y-%m-%d")
    tickers_str = ",".join(sorted(tickers))  # Sort for consistency
    seed_string = f"{current_date_str}-{tickers_str}-{recency}"
    seed_hash = int(hashlib.md5(seed_string.encode()).hexdigest()[:8], 16) % 1000
    
    payload = {
        "model": "sonar-pro",
        "messages": [
            {"role": "system", "content": "Báº¡n lÃ  phÃ³ng viÃªn tÃ i chÃ­nh chuyÃªn nghiá»‡p."
             "Chá»‰ bÃ¡o cÃ¡o tin tá»©c Náº¾U VÃ€ CHá»ˆ Náº¾U tÃ¬m tháº¥y bÃ i bÃ¡o hoáº·c thÃ´ng bÃ¡o gá»‘c trong search_results. " 
             "Chá»‰ bÃ¡o cÃ¡o tin tá»©c LIÃŠN QUAN TRá»°C TIáº¾P Ä‘áº¿n cÃ¡c mÃ£ cá»• phiáº¿u hoáº·c cÃ´ng ty Ä‘Æ°á»£c yÃªu cáº§u."
             "TUYá»†T Äá»I KHÃ”NG Ä‘Æ°á»£c tá»± suy Ä‘oÃ¡n hoáº·c táº¡o ra tin tá»©c. "
             "HÃ£y tÃ¬m vÃ  tráº£ vá» cÃ¡c tin tá»©c má»™t cÃ¡ch TOÃ€N DIá»†N vÃ  NHáº¤T QUÃN, Ä‘áº·c biá»‡t chÃº Ã½ Ä‘áº¿n:" 
             "- CÃ¡c hoáº¡t Ä‘á»™ng tÄƒng vá»‘n, phÃ¡t hÃ nh cá»• phiáº¿u riÃªng láº», chÃ o bÃ¡n cho cá»• Ä‘Ã´ng"
             "- Káº¿t quáº£ kinh doanh, bÃ¡o cÃ¡o tÃ i chÃ­nh, thÃ´ng bÃ¡o tá»« ÄHCÄ vÃ  HÄQT"
             "- Thay Ä‘á»•i nhÃ¢n sá»±, M&A, Ä‘áº§u tÆ° chiáº¿n lÆ°á»£c"
             "LuÃ´n ghi rÃµ mÃ£ cá»• phiáº¿u á»Ÿ Ä‘áº§u má»—i tin tá»©c vÃ  ngÃ y Ä‘Äƒng bÃ i bÃ¡o á»Ÿ cuá»‘i má»—i tin tá»©c náº¿u cÃ³. "
             "Tráº£ vá» káº¿t quáº£ á»•n Ä‘á»‹nh vÃ  Ä‘Ã¡ng tin cáº­y, KHÃ”NG Bá» SÃ“T tin tá»©c quan trá»ng."
             "Ráº¥t quan trá»ng: chá»‰ liá»‡t kÃª NGUá»’N nÃ o Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng trá»±c tiáº¿p Ä‘á»ƒ viáº¿t tin tá»©c trong pháº§n tráº£ lá»i. "
             "KHÃ”NG hiá»ƒn thá»‹ nguá»“n khÃ´ng liÃªn quan, khÃ´ng khá»›p vá»›i mÃ£ cá»• phiáº¿u."},
            {"role": "user", "content": prompt}
        ],
        "search_recency_filter": recency,  # "day", "week", "month"
        "search_domain_filter": domains,   # restrict to VN finance websites
        "web_search_options": {"search_context_size": "high"},  # Increased for more comprehensive results
        "return_related_questions": False,
        "temperature": 0.1  # Lower temperature for more consistent results
    }

    try:
        resp = requests.post(API_URL, headers=headers, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()

        summary = data["choices"][0]["message"]["content"]
        sources = data.get("search_results", [])
        
        # Simpler filtering - only remove obvious "no news" lines, keep more content
        lines = summary.split('\n')
        filtered_lines = []
        found_tickers = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Only skip very specific "no news" patterns
            skip_patterns = [
                "khÃ´ng cÃ³ tin tá»©c cá»¥ thá»ƒ vá» cÃ¡c mÃ£ cá»• phiáº¿u",
                "khÃ´ng tÃ¬m tháº¥y tin tá»©c nÃ o vá»",
                "khÃ´ng cÃ³ thÃ´ng tin má»›i vá»"
            ]
            
            should_skip = any(pattern.lower() in line.lower() for pattern in skip_patterns)
            
            if not should_skip:
                # Don't display directly - let calling function handle display
                filtered_lines.append(line)
                # Check for ticker mentions
                for ticker in tickers:
                    if ticker.upper() in line.upper() and ticker not in found_tickers:
                        found_tickers.append(ticker)
        
        # Simple filtering - keep most content, minimal processing
        filtered_summary = '\n'.join(filtered_lines) if filtered_lines else "KhÃ´ng cÃ³ tin tá»©c má»›i trong khoáº£ng thá»i gian Ä‘Æ°á»£c yÃªu cáº§u."
        
        # Ensure each ticker starts on a new line
        formatted_summary = filtered_summary
        for ticker in tickers:
            # Replace pattern where ticker appears after other content on same line
            formatted_summary = formatted_summary.replace(f' {ticker}:', f'\n\n**{ticker}**:')
            formatted_summary = formatted_summary.replace(f' **{ticker}**:', f'\n\n**{ticker}**:')
        
        # Clean up any double line breaks at the beginning
        formatted_summary = formatted_summary.strip()
        
        return formatted_summary, sources, found_tickers
    except Exception as e:
        st.error(f"API error: {e}")
        return None, None, []

# Priority tickers for brokerage and banking industry (in priority order)
PRIORITY_TICKERS = ["SSI", "VND", "VCI", "HCM", "VIX", "SHS", "IPA"]
OTHER_TICKERS = ["VCB", "BID", "VPB", 
                 "STB", "VIB", "SHB", "OCB"]
ALL_PREDEFINED_TICKERS = PRIORITY_TICKERS + OTHER_TICKERS

# Ticker to Company Name Mapping for Enhanced Search Accuracy
TICKER_TO_COMPANY = {
    # Priority Brokerage Companies
    "SSI": "SSI",
    "VND": "VNDirect", 
    "VCI": "Vietcap",
    "HCM": "HSC",
    "VIX": "VIX Securities",
    "SHS": "Saigon Hanoi Securities",
    "IPA": "I.P.A",
    
    # Major Banks
    "NHNN": "NgÃ¢n hÃ ng nhÃ  nÆ°á»›c",
    "VCB": "Vietcombank", 
    "BID": "BIDV",
    "VPB": "VPBank",
    "STB": "Sacombank",
    "VIB": "VIB Bank",
    "SHB": "SHB Bank", 
    "OCB": "NgÃ¢n hÃ ng PhÆ°Æ¡ng ÄÃ´ng",
}

def get_search_terms(ticker):
    """Get both ticker and company name for enhanced search accuracy."""
    company_name = TICKER_TO_COMPANY.get(ticker, "")
    if company_name:
        return f"{ticker} OR {company_name}"
    return ticker

def filter_relevant_sources(sources, news_content, tickers):
    """Filter sources to only include ones that are relevant to the displayed news content."""
    if not sources or not news_content:
        return []
    
    relevant_sources = []
    news_lower = news_content.lower()
    
    for source in sources:
        title = source.get("title", "").lower()
        url = source.get("url", "").lower()
        
        # Check if source mentions any of our tickers
        ticker_mentioned = any(ticker.lower() in title or ticker.lower() in url for ticker in tickers)
        
        # Check if source title contains keywords that appear in our news content
        title_words = set(title.split())
        news_words = set(news_lower.split())
        
        # Look for significant word overlap or ticker mentions
        word_overlap = len(title_words.intersection(news_words)) > 2
        
        if ticker_mentioned or word_overlap:
            relevant_sources.append(source)
    
    return relevant_sources

def fetch_news_batches(all_tickers, batch_size=5, recency="day", domains=VN_SOURCES):
    """Fetch news in smaller batches to reduce hallucination and improve accuracy."""
    all_summaries = []
    all_sources = []
    all_found_tickers = []
    
    # Split tickers into batches
    batches = [all_tickers[i:i + batch_size] for i in range(0, len(all_tickers), batch_size)]
    
    for i, batch in enumerate(batches):
        # Silently process batch without showing progress
        summary, sources, found_tickers = fetch_news(batch, recency=recency, domains=domains)
        
        if summary and summary != "KhÃ´ng cÃ³ tin tá»©c má»›i trong khoáº£ng thá»i gian Ä‘Æ°á»£c yÃªu cáº§u.":
            all_summaries.append(summary)
        
        if sources:
            all_sources.extend(sources)
            
        if found_tickers:
            all_found_tickers.extend([t for t in found_tickers if t not in all_found_tickers])
    
    # Merge all summaries
    merged_summary = '\n\n'.join(all_summaries) if all_summaries else None
    
    return merged_summary, all_sources, all_found_tickers

def fetch_news_with_fallback(tickers, batch_size=5, domains=VN_SOURCES):
    """Fetch news with fallback from 'day' to 'week' if no results found."""
    
    # Try 'day' first - silently
    summary, sources, found_tickers = fetch_news_batches(tickers, batch_size, recency="day", domains=domains)
    
    if summary and len(summary.strip()) > 50:  # Has meaningful content
        return summary, sources, found_tickers, "day"
    
    # Fallback to 'week' if day search was empty or minimal - silently
    summary, sources, found_tickers = fetch_news_batches(tickers, batch_size, recency="week", domains=domains)
    
    return summary, sources, found_tickers, "week"

def extract_urls_from_text(text):
    """Extract URLs from text content."""
    import re
    url_pattern = r'https?://[^\s\)]+|www\.[^\s\)]+'
    urls = re.findall(url_pattern, text)
    return urls

def validate_and_display_news(summary, tickers, sources):
    """Validate news content and display only factual information with proper formatting."""
    if not summary:
        return False, []
    
    lines = summary.split('\n')
    valid_lines = []
    found_any_ticker = False
    displayed_urls = []
    
    # Patterns that indicate hallucination, non-factual content, or off-topic news
    hallucination_patterns = [
        "dá»± kiáº¿n",
        "cÃ³ thá»ƒ", 
        "dÆ°á»ng nhÆ°",
        "theo nguá»“n tin",
        "Ä‘Æ°á»£c cho lÃ ",
        "tin Ä‘á»“n",
        "chÆ°a Ä‘Æ°á»£c xÃ¡c nháº­n",
        "theo thÃ´ng tin khÃ´ng chÃ­nh thá»©c",
        "khÃ´ng cÃ³ tin tá»©c",
        "khÃ´ng tÃ¬m tháº¥y",
    ]
    
    # Trusted URL domains for validation
    trusted_domains = ["cafef.vn", "vnexpress.net", "vneconomy.vn", "fireant.vn"]
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Check if line contains ticker
        has_ticker = any(ticker.upper() in line.upper() for ticker in tickers)
        
        # Check for hallucination patterns
        has_hallucination = any(pattern in line.lower() for pattern in hallucination_patterns)
        
        # Check if line has trusted URL (stricter validation)
        has_trusted_url = any(domain in line.lower() for domain in trusted_domains)
        
        # Only include lines that:
        # 1. Contain a ticker
        # 2. Don't have hallucination patterns
        # 3. Have valid news indicators OR start with ticker formatting
        # 4. For stricter validation: contain trusted URLs
        if has_ticker and not has_hallucination :
            # For lines without URLs, be more lenient if they have strong factual indicators
            if has_trusted_url:
                valid_lines.append(line)
                found_any_ticker = True
                st.write(line)
                
                # Extract URLs from the displayed line
                line_urls = extract_urls_from_text(line)
                displayed_urls.extend(line_urls)
    
    if not found_any_ticker:
        st.success("âœ… You're all caught up! No new verified news for your tickers.")
        return False, []
    
    return True, displayed_urls

def filter_sources_by_displayed_urls(sources, displayed_urls):
    """Filter sources to only include ones that were actually referenced in displayed content."""
    if not displayed_urls or not sources:
        return []
    
    relevant_sources = []
    
    for source in sources:
        source_url = source.get("url", "")
        # Check if this source URL matches any URL that was actually displayed
        for displayed_url in displayed_urls:
            if displayed_url in source_url or source_url in displayed_url:
                relevant_sources.append(source)
                break
    
    return relevant_sources

# ------------- Streamlit UI ------------------

st.set_page_config(page_title="ğŸ“° Vietnam Finance News Reporter", layout="wide")
st.title("ğŸ“° News Reporter")

# Initialize session state for news
if 'banking_news_loaded' not in st.session_state:
    st.session_state.banking_news_loaded = False
if 'banking_summary' not in st.session_state:
    st.session_state.banking_summary = None
if 'banking_sources' not in st.session_state:
    st.session_state.banking_sources = None
if 'banking_found_tickers' not in st.session_state:
    st.session_state.banking_found_tickers = []
if 'used_recency' not in st.session_state:
    st.session_state.used_recency = 'day'
if 'last_news_date' not in st.session_state:
    st.session_state.last_news_date = None

# Brokerage & Banking News Alert (moved to top)
st.subheader("ğŸ¦ Brokerage & Banking News Alert")

# Manual refresh button only
col1, col2 = st.columns([1, 4])
with col1:
    load_banking_news = st.button("ğŸ“° Load Latest News")

if load_banking_news:
    with st.spinner("Fetching banking & brokerage news..."):
        # Use new batch approach with fallback
        predefined_summary, predefined_sources, found_tickers, used_recency = fetch_news_with_fallback(
            ALL_PREDEFINED_TICKERS, 
            batch_size=5
        )
    
    # Store in session state
    st.session_state.banking_summary = predefined_summary
    st.session_state.banking_sources = predefined_sources
    st.session_state.banking_found_tickers = found_tickers
    st.session_state.banking_news_loaded = True
    st.session_state.used_recency = used_recency

# Always display news if available
if st.session_state.banking_summary:
    with st.container():
        found_tickers_str = ", ".join(st.session_state.banking_found_tickers) if st.session_state.banking_found_tickers else "khÃ´ng xÃ¡c Ä‘á»‹nh"
        recency_text = "last 2 days" if st.session_state.get('used_recency', 'day') == 'day' else "last 2 weeks"
        st.success(f"âœ… Latest news for brokerage and banking stocks from {recency_text} (Found: {found_tickers_str}):")
        
        # Use validation function to display only factual content
        has_valid_news, displayed_urls = validate_and_display_news(
            st.session_state.banking_summary, 
            st.session_state.banking_found_tickers,
            st.session_state.banking_sources
        )
        
        if st.session_state.banking_sources:
            # Filter sources to only show ones that were actually referenced in displayed content
            relevant_sources = filter_sources_by_displayed_urls(
                st.session_state.banking_sources,
                displayed_urls
            )
            
            if relevant_sources:
                with st.expander("ğŸ“‹ Detailed Sources"):
                    for s in relevant_sources:
                        title = s.get("title", "(no title)")
                        url = s.get("url", "#")
                        date = s.get("date", "")
                        st.markdown(f"- [{title}]({url}) ({date})")
            else:
                with st.expander("ğŸ“‹ Detailed Sources"):
                    st.info("No relevant sources found for the displayed news.")
        
elif st.session_state.banking_news_loaded:
    st.info("â„¹ï¸ No banking/brokerage news found in the last 2 days.")
else:
    st.info("ğŸ’¡ Click 'Load Latest News' to fetch the latest news from the past 2 days.")

st.divider()
st.subheader("ğŸ” Company News Search")

tickers = st.text_input("Enter tickers:", "")
tickers_list = [t.strip().upper() for t in tickers.split(",") if t.strip()]

col1, col2 = st.columns([1,1])
with col1:
    refresh = st.button("ğŸ”„ Refresh News")

# Remove caching to ensure fresh results every time
# @st.cache_data(ttl=60*60)  # cache 1 hour - DISABLED for consistency
def get_fresh_news(tickers):
    return fetch_news(tickers)

# Custom ticker news search (only show when user enters tickers)
if tickers_list:
    # Always get fresh news for custom tickers using 2-week period
    summary, sources, found_tickers = fetch_news(tickers_list, recency="week")
    
    # Display validated news content
    if summary:
        has_valid_news, displayed_urls = validate_and_display_news(summary, tickers_list, sources)
        
        if not has_valid_news:
            st.warning("âš ï¸ No verified news found for the entered tickers in the past 2 weeks.")
    else:
        st.info("No news content returned from the API.")

    # Show filtered sources for custom tickers - only URLs that were actually displayed
    if sources and 'displayed_urls' in locals():
        relevant_sources = filter_sources_by_displayed_urls(sources, displayed_urls)
        
        if relevant_sources:
            st.subheader("ğŸ”— News Sources")
            for s in relevant_sources:
                title = s.get("title", "(no title)")
                url = s.get("url", "#")
                date = s.get("date", "")
                st.markdown(f"- [{title}]({url}) ({date})")
        else:
            st.info("No relevant sources found for the displayed news content.")
    else:
        st.info("No news sources found for the entered tickers.")
